import os
import shutil
import uuid
import requests
import psycopg2
import json
from datetime import datetime
from typing import List, Dict, Any, Optional
from difflib import SequenceMatcher

from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware

# Use PyMuPDF loader instead of Docling
from langchain_community.document_loaders import PyMuPDFLoader

from langchain_core.embeddings import Embeddings
from langchain_community.vectorstores.pgvector import PGVector
from langchain_openai import ChatOpenAI
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.documents import Document

# === Config ===
EMBEDDING_URL = "http://localhost:1234/v1/embeddings"
MODEL_NAME = "qwen3-4b"
COLLECTION_NAME = "hierarchical_embeddings"
PG_CONN_STRING = "postgresql+psycopg2://postgres:12345@localhost:5432/chat_history"
TEMP_UPLOAD_DIR = "temp_uploads"

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"]
)

# Ensure temp directory exists
os.makedirs(TEMP_UPLOAD_DIR, exist_ok=True)

def get_db_connection():
    try:
        return psycopg2.connect(
            dbname="chat_history",
            user="postgres",
            password="12345",
            host="localhost",
            connect_timeout=5
        )
    except Exception as e:
        print(f"Database connection error: {e}")
        raise HTTPException(status_code=500, detail="Database connection failed")

# Custom Embeddings with retry logic
class CustomEmbeddings(Embeddings):
    def __init__(self):
        self.max_retries = 3
        self.timeout = 600

    def _make_embedding_request(self, texts, is_query=False):
        for attempt in range(self.max_retries):
            try:
                res = requests.post(
                    EMBEDDING_URL,
                    headers={"Content-Type": "application/json"},
                    json={
                        "input": [texts] if is_query else texts,
                        "model": "nomic-embed-text-v1"
                    },
                    timeout=self.timeout
                )
                res.raise_for_status()
                return res.json()
            except Exception as e:
                if attempt == self.max_retries - 1:
                    print(f"Embedding final attempt failed: {e}")
                    raise
                print(f"Embedding attempt {attempt + 1} failed, retrying...")

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        try:
            result = self._make_embedding_request(texts)
            return [item["embedding"] for item in result["data"]]
        except Exception as e:
            print(f"Embedding documents error: {e}")
            raise HTTPException(status_code=500, detail="Embedding service failed")

    def embed_query(self, text: str) -> List[float]:
        try:
            result = self._make_embedding_request(text, is_query=True)
            return result["data"][0]["embedding"]
        except Exception as e:
            print(f"Embedding query error: {e}")
            raise HTTPException(status_code=500, detail="Embedding service failed")

embedding_function = CustomEmbeddings()

llm = ChatOpenAI(
    model_name=MODEL_NAME,
    openai_api_base="http://localhost:1234/v1",
    openai_api_key="not-needed",
    temperature=0.7,
    max_retries=3,
    request_timeout=600
)

# ======================================
# Hierarchical chunking using PyMuPDF
# ======================================
class HierarchicalChunker:
    """
    Use PyMuPDFLoader to extract text, then apply a custom hierarchical chunking
    heuristic. No fallback to recursive splitters â€” if loader fails we raise.
    """
    def __init__(self):
        # no heavy init needed
        pass

    def process_document(self, file_path: str) -> List[Document]:
        # Load PDF pages via PyMuPDFLoader
        try:
            loader = PyMuPDFLoader(file_path)
            documents = loader.load()  # returns list of Document (each page usually)
        except Exception as e:
            print(f"PyMuPDFLoader error: {e}")
            raise HTTPException(status_code=500, detail=f"PDF load error: {str(e)}")

        hierarchical_chunks: List[Document] = []
        for page_index, doc in enumerate(documents):
            page_text = doc.page_content or ""
            # Normalize newlines
            page_text = page_text.replace("\r\n", "\n").strip()

            # Split page into blocks (paragraph-like) by double newlines
            blocks = [b.strip() for b in page_text.split("\n\n") if b.strip()]

            # If no double-newline blocks, split by single newline lines grouped
            if not blocks and page_text:
                # group lines into paragraphs by empty-line heuristics
                lines = [l.strip() for l in page_text.split("\n") if l.strip()]
                # join lines into one block for simplicity
                blocks = ["\n".join(lines)]

            for block_idx, block in enumerate(blocks):
                # Heuristically detect heading vs paragraph
                level = self._infer_hierarchy_level(block)
                metadata = {
                    "source": file_path,
                    "element_type": "heading" if level <= 4 else "paragraph",
                    "page_number": page_index + 1,
                    "hierarchy_level": level,
                    "element_id": str(uuid.uuid4())
                }

                # Create chunks from this block based on hierarchy level
                chunks = self._create_hierarchical_chunks(block, metadata)
                hierarchical_chunks.extend(chunks)

        if not hierarchical_chunks:
            raise HTTPException(status_code=500, detail="No hierarchical chunks produced from PDF")

        return hierarchical_chunks

    def _infer_hierarchy_level(self, text_block: str) -> int:
        """
        Heuristic rules to detect headings and assign hierarchy level:
        - Very short lines (<= 6 words) and either Title Case or UPPER => heading (levels 1-3)
        - Lines ending with ':' often are headings
        - Otherwise paragraph (level 5)
        Returns 1..7 (lower = higher importance)
        """
        stripped = text_block.strip()
        # first line heuristics
        first_line = stripped.split("\n")[0].strip()
        word_count = len(first_line.split())
        is_all_caps = first_line.replace(" ", "").isupper()
        is_title_like = first_line.istitle() and word_count <= 8
        ends_with_colon = first_line.endswith(":")

        # very short and all-caps -> top heading
        if (word_count <= 6 and is_all_caps) or (word_count <= 4 and ends_with_colon):
            return 1
        # title-like small -> heading level 2
        if is_title_like or (word_count <= 8 and ends_with_colon):
            return 2
        # moderate short -> heading level 3
        if word_count <= 12 and word_count > 8:
            return 3
        # otherwise paragraph
        return 5

    def _create_hierarchical_chunks(self, text: str, metadata: Dict) -> List[Document]:
        """
        Same chunking logic as your previous code but adapted for blocks from PyMuPDF.
        """
        chunks: List[Document] = []
        chunk_sizes = {
            1: 1500, 2: 1200, 3: 1000, 4: 800, 5: 600, 6: 400, 7: 300
        }

        level = metadata.get('hierarchy_level', 5)
        chunk_size = chunk_sizes.get(level, 600)
        overlap_size = min(100, chunk_size // 6)

        words = text.split()
        current_chunk: List[str] = []
        current_size = 0

        for word in words:
            word_size = len(word) + 1
            if current_size + word_size > chunk_size and current_chunk:
                chunk_text = ' '.join(current_chunk)
                chunk_metadata = metadata.copy()
                chunk_metadata.update({
                    'chunk_size': len(chunk_text),
                    'chunk_index': len(chunks)
                })
                chunks.append(Document(page_content=chunk_text, metadata=chunk_metadata))

                # approximate overlap by characters -> pick words from end until chars >= overlap_size
                if overlap_size > 0:
                    overlap_words = []
                    chars = 0
                    for w in reversed(current_chunk):
                        overlap_words.insert(0, w)
                        chars += len(w) + 1
                        if chars >= overlap_size:
                            break
                else:
                    overlap_words = []

                current_chunk = overlap_words + [word]
                current_size = sum(len(w) + 1 for w in current_chunk)
            else:
                current_chunk.append(word)
                current_size += word_size

        if current_chunk:
            chunk_text = ' '.join(current_chunk)
            chunk_metadata = metadata.copy()
            chunk_metadata.update({
                'chunk_size': len(chunk_text),
                'chunk_index': len(chunks)
            })
            chunks.append(Document(page_content=chunk_text, metadata=chunk_metadata))

        return chunks

# -------------------------
# Database Operations
# -------------------------
def save_hierarchical_chunks(chunks: List[Document], session_id: str):
    conn = None
    try:
        conn = get_db_connection()
        cur = conn.cursor()

        for chunk in chunks:
            chunk_id = str(uuid.uuid4())
            metadata_json = json.dumps(chunk.metadata)

            cur.execute(
                """INSERT INTO hierarchical_documents 
                (id, session_id, content, metadata, hierarchy_level, element_type, created_at) 
                VALUES (%s, %s, %s, %s, %s, %s, %s)""",
                (
                    chunk_id,
                    session_id,
                    chunk.page_content,
                    metadata_json,
                    chunk.metadata.get('hierarchy_level', 5),
                    chunk.metadata.get('element_type', 'paragraph'),
                    datetime.utcnow()
                )
            )

        conn.commit()
    except Exception as e:
        print(f"Error saving chunks: {e}")
        if conn:
            conn.rollback()
        raise HTTPException(status_code=500, detail=f"Error saving chunks: {str(e)}")
    finally:
        if conn:
            conn.close()

def save_message(session_id: str, role: str, content: str):
    conn = None
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        cur.execute(
            "INSERT INTO chat_messages (session_id, role, content) VALUES (%s, %s, %s)",
            (session_id, role, content)
        )
        conn.commit()
    except Exception as e:
        print(f"Error saving message: {e}")
        raise
    finally:
        if conn:
            conn.close()

def save_qa(session_id: str, question: str, answer: str):
    conn = None
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        cur.execute(
            "INSERT INTO chat_qa (id, session_id, question, answer) VALUES (%s, %s, %s, %s)",
            (str(uuid.uuid4()), session_id, question, answer)
        )
        conn.commit()
    except Exception as e:
        print(f"Error saving Q&A: {e}")
        raise
    finally:
        if conn:
            conn.close()

def fetch_chat_history(session_id: str, limit: int = 10) -> List[Any]:
    conn = None
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        cur.execute(
            "SELECT role, content FROM chat_messages "
            "WHERE session_id = %s ORDER BY timestamp DESC LIMIT %s",
            (session_id, limit)
        )
        rows = cur.fetchall()
        rows.reverse()

        history = []
        for role, content in rows:
            if role == "human":
                history.append(HumanMessage(content))
            else:
                history.append(AIMessage(content))
        return history
    except Exception as e:
        print(f"Error fetching chat history: {e}")
        return []
    finally:
        if conn:
            conn.close()

def find_similar_question(session_id: str, current_query: str, threshold: float = 0.85) -> Optional[str]:
    conn = None
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        cur.execute(
            "SELECT question, answer FROM chat_qa WHERE session_id = %s",
            (session_id,)
        )
        qas = cur.fetchall()

        for past_q, past_a in qas:
            similarity = SequenceMatcher(None, current_query.lower(), past_q.lower()).ratio()
            if similarity >= threshold:
                return past_a
        return None
    except Exception as e:
        print(f"Error finding similar questions: {e}")
        return None
    finally:
        if conn:
            conn.close()

# -------------------------
# API Endpoints
# -------------------------
@app.post("/upload_pdf")
async def upload_pdf(file: UploadFile = File(...), session_id: str = Form(...)):
    try:
        if not file.filename.lower().endswith('.pdf'):
            raise HTTPException(status_code=400, detail="Only PDF files are supported")

        file_path = os.path.join(TEMP_UPLOAD_DIR, f"temp_{uuid.uuid4().hex}.pdf")

        try:
            with open(file_path, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to save file: {str(e)}")

        # Use PyMuPDF-based hierarchical chunker (no fallback)
        chunker = HierarchicalChunker()
        hierarchical_chunks = chunker.process_document(file_path)

        # Save chunks to DB
        save_hierarchical_chunks(hierarchical_chunks, session_id)

        # Store in pgvector
        vectorstore = PGVector(
            connection_string=PG_CONN_STRING,
            collection_name=COLLECTION_NAME,
            embedding_function=embedding_function
        )

        if hierarchical_chunks:
            vectorstore.add_documents(hierarchical_chunks)

        try:
            os.remove(file_path)
        except:
            pass

        return JSONResponse({
            "message": f"PDF processed successfully with {len(hierarchical_chunks)} hierarchical chunks",
            "chunks_created": len(hierarchical_chunks)
        })

    except HTTPException:
        raise
    except Exception as e:
        print(f"Unhandled upload error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat")
async def chat_endpoint(data: dict):
    try:
        session_id = data.get("session_id")
        user_input = data.get("query")

        if not session_id or not user_input:
            raise HTTPException(status_code=400, detail="Missing session_id or query")

        similar_answer = find_similar_question(session_id, user_input)
        if similar_answer:
            save_message(session_id, "human", user_input)
            save_message(session_id, "ai", similar_answer)
            return JSONResponse({"answer": similar_answer})

        chat_history = fetch_chat_history(session_id, limit=10)

        vectorstore = PGVector(
            connection_string=PG_CONN_STRING,
            collection_name=COLLECTION_NAME,
            embedding_function=embedding_function
        )

        retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

        contextualize_prompt = ChatPromptTemplate.from_messages([
            ("system", """Given a chat history and the latest user question, 
            reformulate it as a standalone question that can be understood without the chat history. 
            Preserve the original intent and important details."""),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}")
        ])

        qa_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a helpful AI assistant. Use the following context from the documents to answer the question.
            Context:
            {context}"""),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}")
        ])

        history_aware_retriever = create_history_aware_retriever(
            llm, retriever, contextualize_prompt
        )

        qa_chain = create_stuff_documents_chain(llm, qa_prompt)
        rag_chain = create_retrieval_chain(history_aware_retriever, qa_chain)

        result = rag_chain.invoke({
            "input": user_input,
            "chat_history": chat_history
        })

        answer = result.get("answer") if isinstance(result, dict) else None
        if not answer:
            if isinstance(result, dict) and "output_text" in result:
                answer = result["output_text"]
            else:
                answer = "I couldn't generate a response."

        save_message(session_id, "human", user_input)
        save_message(session_id, "ai", answer)
        save_qa(session_id, user_input, answer)

        return JSONResponse({"answer": answer})

    except Exception as e:
        print(f"Chat error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/chat_history/{session_id}")
async def get_chat_history(session_id: str, limit: int = 50):
    conn = None
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        cur.execute(
            "SELECT role, content, timestamp FROM chat_messages "
            "WHERE session_id = %s ORDER BY timestamp ASC LIMIT %s",
            (session_id, limit)
        )
        rows = cur.fetchall()

        history = []
        for role, content, timestamp in rows:
            history.append({
                "role": role,
                "content": content,
                "timestamp": timestamp.isoformat()
            })

        return JSONResponse({"history": history})

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        if conn:
            conn.close()

@app.delete("/chat_history/{session_id}")
async def clear_chat_history(session_id: str):
    conn = None
    try:
        conn = get_db_connection()
        cur = conn.cursor()

        cur.execute("DELETE FROM chat_messages WHERE session_id = %s", (session_id,))
        message_count = cur.rowcount

        cur.execute("DELETE FROM chat_qa WHERE session_id = %s", (session_id,))
        qa_count = cur.rowcount

        conn.commit()

        return JSONResponse({
            "message": f"Cleared {message_count} messages and {qa_count} Q&A pairs"
        })

    except Exception as e:
        if conn:
            conn.rollback()
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        if conn:
            conn.close()

@app.get("/health")
async def health_check():
    try:
        # Test database connection
        conn = get_db_connection()
        conn.close()

        # Test embedding service
        test_embed = embedding_function.embed_query("test")
        if not test_embed or len(test_embed) == 0:
            raise Exception("Embedding service returned empty response")

        return JSONResponse({"status": "healthy"})
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
