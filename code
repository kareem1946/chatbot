import os
import psycopg2
import re
from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings
from langchain_community.vectorstores.pgvector import PGVector
from langchain_community.llms.openai import OpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain_core.messages import HumanMessage
from langchain_core.prompts import PromptTemplate
from langchain_community.chat_models import ChatOpenAI
from langchain_community.llms import HuggingFaceEndpoint
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import PGVector
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings
from langchain_community.vectorstores.pgvector import PGVector
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain_community.llms.openai import OpenAI
from langchain_community.chat_models import ChatOpenAI
from langchain_community.llms import HuggingFaceEndpoint
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain.vectorstores import PGVector
from langchain.memory import ConversationBufferMemory
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough


os.environ["OPENAI_API_BASE"] = "http://localhost:1234/v1"
os.environ["OPENAI_API_KEY"] = "lm-studio"  


loader = PyPDFLoader("testpdf.pdf")
docs = loader.load()


text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
all_splits = text_splitter.split_documents(docs)


embeddings = HuggingFaceInferenceAPIEmbeddings(
    api_key="not-needed", 
    inference_api_url="http://localhost:1234/v1/embeddings"
)


CONNECTION_STRING = "postgresql://postgres:12345@localhost:5432/langchain_chatbot"
vectorstore = PGVector.from_documents(
    documents=all_splits,
    embedding=embeddings,
    collection_name="embeddings",
    connection_string=CONNECTION_STRING,
)


llm = ChatOpenAI(
    model="lmstudio",  
    temperature=0,
)


prompt_template = """
You are an assistant answering user questions based on the following context:

{context}

Chat history:
{chat_history}

Question:
{question}
"""

prompt = PromptTemplate(
    input_variables=["context", "chat_history", "question"],
    template=prompt_template,
)


memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)


qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    memory=memory,
    combine_docs_chain_kwargs={"prompt": prompt}
)


print(" Chatbot is ready. Type 'exit' to quit.")

while True:
    user_input = input("You: ")
    if user_input.lower() in ["exit", "quit"]:
        break

    result = qa_chain({"question": user_input})
    raw_answer = result["answer"]

    cleaned_answer = re.sub(r"<think>.*?</think>", "", raw_answer, flags=re.DOTALL).strip()

    print(f"Bot: {cleaned_answer}")
